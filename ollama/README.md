Structured LLM Output https://youtu.be/-TXPG-KbQhk

Hosting models locally using Ollama https://youtu.be/IGox9gOCtwU
