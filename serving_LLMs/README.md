# Frmaeworks for serving LLMs

- (lorax)[https://github.com/predibase/lorax] employs a number of optimization techniques like
   + kv caching
   + batching
   + continuous batching
   + quantization (for optimizing memory footprint)
   + LoRA
   + multiple LoRA
- (vllm)[https://github.com/vllm-project/vllm]
- h2oGPT
- gpt4all
- llm
- ollama
- PrivateGPT
