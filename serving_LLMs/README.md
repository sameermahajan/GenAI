# Frameworks for serving LLMs

- [lorax](https://github.com/predibase/lorax) employs a number of optimization techniques like
   + kv caching
   + batching
   + continuous batching
   + quantization (for optimizing memory footprint)
   + LoRA
   + multiple LoRA
- [vllm](https://github.com/vllm-project/vllm)
- [h2oGPT](https://github.com/h2oai/h2ogpt)
- gpt4all
- llm
- ollama
- PrivateGPT
