# Frameworks for serving LLMs

[blog](https://sameermahajan.medium.com/13-ways-of-serving-llms-for-inferencing-b8a82ad0cf19) with some details

- [lorax](https://github.com/predibase/lorax) employs a number of optimization techniques like
   + kv caching
   + batching
   + continuous batching
   + quantization (for optimizing memory footprint)
   + LoRA
   + multiple LoRA
- [vllm](https://github.com/vllm-project/vllm)
- [h2oGPT](https://github.com/h2oai/h2ogpt)
- gpt4all
- llm
- ollama
- PrivateGPT
